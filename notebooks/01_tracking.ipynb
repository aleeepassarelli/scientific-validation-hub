{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5zZw4yxj6srWt3fNa1/dG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleeepassarelli/scientific-validation-hub/blob/main/notebooks/01_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§ª Lab 01: Experiment Tracking com MLflow (Serverless)\n",
        "\n",
        "**Framework:** Scientific Validation Hub\n",
        "**Ferramenta:** [MLflow](https://mlflow.org/)\n",
        "**Objetivo:** Rastrear versÃµes de prompts e mÃ©tricas de performance sem precisar configurar servidores complexos.\n",
        "\n",
        "---\n",
        "\n",
        "### âš¡ Quick Start (Modo Assistido)\n",
        "1.  **Execute:** Rode todas as cÃ©lulas abaixo (â–¶) para simular uma bateria de testes de um Agente.\n",
        "2.  **Analise:** No final, geraremos uma tabela (DataFrame) com o histÃ³rico.\n",
        "3.  **Audite:** Abra o Assistente Gemini e cole:\n",
        "\n",
        "> \"Atue como Cientista de Dados SÃªnior.\n",
        "> Analise a tabela de experimentos 'results_df' gerada abaixo.\n",
        "> 1. Qual versÃ£o do prompt ('prompt_version') teve a melhor relaÃ§Ã£o entre 'quality_score' e 'latency_ms'?\n",
        "> 2. Existe correlaÃ§Ã£o entre a 'temperature' e o 'quality_score'?\n",
        "> 3. Gere um insight sobre qual configuraÃ§Ã£o devemos levar para produÃ§Ã£o.\""
      ],
      "metadata": {
        "id": "9nrlExqWK_iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalando MLflow e dependÃªncias de anÃ¡lise\n",
        "!pip install mlflow pandas matplotlib -q\n",
        "print(\"âœ… MLflow instalado. Pronto para iniciar o laboratÃ³rio.\")"
      ],
      "metadata": {
        "id": "qNnqKBmBK78p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJWeTl6MKtgb"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "# 1. ConfiguraÃ§Ã£o do MLflow Local (Sem Servidor)\n",
        "# Isso cria uma pasta ./mlruns no Colab para salvar os dados\n",
        "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
        "experiment_name = \"Validacao_Agente_Vendas\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"ðŸ“ Tracking URI configurado: file:./mlruns\")\n",
        "print(f\"ðŸ§ª Experimento ativo: {experiment_name}\")\n",
        "\n",
        "# 2. Simulador de Agente (Mock)\n",
        "# Em um caso real, aqui vocÃª chamaria a API do Gemini/OpenAI\n",
        "def test_agent_performance(prompt_ver, model_type, temp):\n",
        "    # SimulaÃ§Ã£o: Modelos mais caros e temperaturas mÃ©dias dÃ£o melhores resultados\n",
        "    base_score = 0.85 if \"gpt-4\" in model_type else 0.65\n",
        "\n",
        "    # Penalidade por temperatura extrema (muito baixa = robÃ³tico, muito alta = alucinaÃ§Ã£o)\n",
        "    temp_penalty = abs(0.5 - temp) * 0.2\n",
        "\n",
        "    # RuÃ­do aleatÃ³rio (variÃ¢ncia natural da IA)\n",
        "    noise = random.uniform(-0.05, 0.05)\n",
        "\n",
        "    final_score = base_score - temp_penalty + noise\n",
        "\n",
        "    # SimulaÃ§Ã£o de LatÃªncia e Tokens\n",
        "    latency = random.randint(800, 2500) if \"gpt-4\" in model_type else random.randint(200, 800)\n",
        "    tokens = 150 + random.randint(-20, 50)\n",
        "\n",
        "    return {\n",
        "        \"quality\": min(1.0, max(0.0, final_score)), # Clamp 0-1\n",
        "        \"latency\": latency,\n",
        "        \"tokens\": tokens\n",
        "    }\n",
        "\n",
        "# 3. Bateria de Testes (Grid Search Simplificado)\n",
        "configs = [\n",
        "    {\"ver\": \"v1_agressiva\", \"model\": \"gpt-3.5-turbo\", \"temp\": 0.9},\n",
        "    {\"ver\": \"v1_agressiva\", \"model\": \"gpt-4\",         \"temp\": 0.9},\n",
        "    {\"ver\": \"v2_consultiva\", \"model\": \"gpt-3.5-turbo\", \"temp\": 0.2},\n",
        "    {\"ver\": \"v2_consultiva\", \"model\": \"gpt-4\",         \"temp\": 0.5}, # Sweet spot teÃ³rico\n",
        "    {\"ver\": \"v3_minimalista\", \"model\": \"gpt-4\",        \"temp\": 0.1},\n",
        "]\n",
        "\n",
        "print(\"\\nðŸš€ Iniciando execuÃ§Ãµes...\")\n",
        "\n",
        "for i, conf in enumerate(configs):\n",
        "    run_name = f\"run_{i}_{conf['ver']}\"\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        # A. Log de ParÃ¢metros (O que mudamos)\n",
        "        mlflow.log_param(\"prompt_version\", conf[\"ver\"])\n",
        "        mlflow.log_param(\"model\", conf[\"model\"])\n",
        "        mlflow.log_param(\"temperature\", conf[\"temp\"])\n",
        "\n",
        "        # B. ExecuÃ§Ã£o\n",
        "        print(f\"   --> Testando: {conf['ver']} no {conf['model']}...\")\n",
        "        metrics = test_agent_performance(conf[\"ver\"], conf[\"model\"], conf[\"temp\"])\n",
        "\n",
        "        # C. Log de MÃ©tricas (O resultado)\n",
        "        mlflow.log_metric(\"quality_score\", metrics[\"quality\"])\n",
        "        mlflow.log_metric(\"latency_ms\", metrics[\"latency\"])\n",
        "        mlflow.log_metric(\"cost_tokens\", metrics[\"tokens\"])\n",
        "\n",
        "        # D. Log de Artefatos (Opcional - ex: o texto gerado)\n",
        "        with open(\"output.txt\", \"w\") as f:\n",
        "            f.write(f\"SimulaÃ§Ã£o de output para {conf['ver']}\")\n",
        "        mlflow.log_artifact(\"output.txt\")\n",
        "\n",
        "print(\"\\nâœ… Ciclo de experimentos finalizado.\")"
      ]
    }
  ]
}